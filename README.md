# Adversarial Attacks on CAPTCHA Recognition Systems

A research project exploring the vulnerability of CAPTCHA recognition models to adversarial attacks. This project implements and compares multiple adversarial attack methods to evaluate CAPTCHA security.

## ğŸ“‹ Overview

This project investigates how adversarial examples can fool AI-based CAPTCHA solvers while remaining recognizable to humans. We implement several state-of-the-art attack techniques and analyze their effectiveness against trained CAPTCHA recognition models.

## ğŸ¯ Features

- **CAPTCHA Recognition Model**: Deep learning model trained to solve text-based CAPTCHAs
- **Multiple Attack Methods**:
  - FGSM (Fast Gradient Sign Method)
  - PGD (Projected Gradient Descent)
  - Carlini-Wagner (C&W) Attack
  - MNIST-based adversarial examples
- **Comprehensive Analysis**:
  - Accuracy vs Confidence plots
  - Confusion matrices
  - Attack success rate visualization
  - Side-by-side comparison of original vs adversarial images

## ğŸ› ï¸ Technologies Used

- Python 3.x
- TensorFlow/Keras
- PyTorch
- NumPy
- Matplotlib
- OpenCV

## ğŸ“ Project Structure
```
â”œâ”€â”€ app.py, app1.py, app2.py, app3.py  # Web applications
â”œâ”€â”€ captcha_model.h5                    # Trained Keras model
â”œâ”€â”€ model.pth                           # Trained PyTorch model
â”œâ”€â”€ label_encoder.pkl                   # Label encoding for predictions
â”œâ”€â”€ attack_results/                     # Generated adversarial examples
â”œâ”€â”€ data/                               # Training data
â”œâ”€â”€ samples/                            # Sample CAPTCHA images
â”œâ”€â”€ *.ipynb                             # Jupyter notebooks for experiments
â””â”€â”€ Project doc.docx                    # Project documentation
```

## ğŸš€ Installation

1. Clone the repository:
```bash
git clone https://github.com/AyushWaman843/Adversarial-Attacks-On-CAPTCHA.git
cd Adversarial-Attacks-On-CAPTCHA
```

2. Install required packages:
```bash
pip install -r requirements.txt
```

## ğŸ’» Usage

### Run Web Application
```bash
python app.py
```

### Run Adversarial Attacks
```bash
# FGSM Attack
python adv_attk_MNIST.ipynb

# Carlini-Wagner Attack
python cw_attack_accuracy_summary.png

# PGD Attack
python pgd_examples.png
```

### Test CAPTCHA Recognition
```bash
python test.ipynb
```

## ğŸ“Š Results

The project generates various visualizations:
- **Confusion Matrices**: Show misclassification patterns
- **Accuracy Plots**: Compare model performance on clean vs adversarial examples
- **Example Comparisons**: Visual side-by-side of original and attacked CAPTCHAs

## ğŸ”¬ Attack Methods Explained

### FGSM (Fast Gradient Sign Method)
Fast single-step attack that adds noise in the direction of the gradient.

### PGD (Projected Gradient Descent)
Iterative attack that takes multiple small steps to create adversarial examples.

### Carlini-Wagner (C&W)
Optimization-based attack that minimizes perturbation while maximizing misclassification.

## ğŸ“ˆ Key Findings

- Adversarial attacks can significantly reduce CAPTCHA recognition accuracy
- C&W attacks produce more subtle perturbations than FGSM
- Trade-off between attack strength and image quality
- Human readability remains intact even with successful attacks

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## ğŸ“„ License

This project is for educational and research purposes only.

## ğŸ‘¤ Author

**Ayush Waman**
- GitHub: [@AyushWaman843](https://github.com/AyushWaman843)

## ğŸ™ Acknowledgments

- Research papers on adversarial machine learning
- CAPTCHA datasets and benchmarks
- Open-source deep learning community

## âš ï¸ Disclaimer

This project is intended for educational and research purposes to improve CAPTCHA security. Do not use these techniques for malicious purposes or to bypass security systems without authorization.

---

**Note**: This project demonstrates security vulnerabilities in automated CAPTCHA solvers and aims to contribute to the development of more robust authentication systems.
